if cache:
if isinstance(cache, str): ds = ds.cache(cache)
else:
ds = ds.cache()
ds = ds.shuffle(buffer_size=shuffle_buffer_size) return ds

test_ds = test_ds.map(process_path)
test_ds = prepare_for_testing(test_ds, cache="test-cached-data")
# convert testing set to numpy array to fit in memory (don't do that when testing
# set is too large)
y_test = np.zeros((n_testing_samples,))
X_test = np.zeros((n_testing_samples, 299, 299, 3))
for i, (img, label) in enumerate(test_ds.take(n_testing_samples)): # print(img.shape, label.shape)
X_test[i] = img y_test[i] = label.numpy()

print("y_test.shape:", y_test.shape) # load the weights with the least loss
m.load_weights("benign-vs-malignant_64_rmsprop_0.390.h5") print("Evaluating the model...")
loss, accuracy = m.evaluate(X_test, y_test, verbose=0) print("Loss:", loss, " Accuracy:", accuracy)
def get_predictions(threshold=None): """
Returns predictions for binary classification given `threshold`
For instance, if threshold is 0.3, then it'll output 1 (malignant) for that sample if
the probability of 1 is 30% or more (instead of 50%) """
y_pred = m.predict(X_test) if not threshold:
threshold = 0.5
result = np.zeros((n_testing_samples,)) for i in range(n_testing_samples):
# test melanoma probability if y_pred[i][0] >= threshold:
result[i] = 1
# else, it's 0 (benign) return result

threshold = 0.23
# get predictions with 23% threshold
# which means if the model is 23% sure or more that is malignant, # it's assigned as malignant, otherwise it's benign
y_pred = get_predictions(threshold)
# print it print(cmn)
fig, ax = plt.subplots(figsize=(10,10)) sns.heatmap(cmn, annot=True, fmt='.2f',
xticklabels=[f"pred_{c}" for c in class_names], yticklabels=[f"true_{c}" for c in class_names], cmap="Blues"
)
plt.ylabel('Actual') plt.xlabel('Predicted')
# plot the resulting confusion matrix plt.show()

plot_confusion_matrix(y_test, y_pred)
def plot_roc_auc(y_true, y_pred): """
This function plots the ROC curves and provides the scores. """
# prepare for figure plt.figure()
fpr, tpr, _ = roc_curve(y_true, y_pred) # obtain ROC AUC
roc_auc = auc(fpr, tpr) # print score
print(f"ROC AUC: {roc_auc:.3f}") # plot ROC curve
plt.plot(fpr, tpr, color="blue", lw=2,
label='ROC curve (area = {f:.2f})'.format(d=1,
f=roc_auc))
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('ROC curves') plt.legend(loc="lower right") plt.show()

plot_roc_auc(y_test, y_pred)
